{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "from revChatGPT.V1 import Chatbot\n",
    "\n",
    "env_path=\"../src/.env\"\n",
    "load_dotenv(dotenv_path=env_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limits\n",
    "\n",
    "OpenAI: 50 requests / hour for each account"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication method\n",
    "### - Access token\n",
    "https://chat.openai.com/api/auth/session\n",
    "```json\n",
    "{\n",
    "  \"access_token\": \"<access_token>\"\n",
    "}\n",
    "```\n",
    "\n",
    "### - Optional configuration:\n",
    "```json\n",
    "{\n",
    "  \"conversation_id\": \"UUID...\",\n",
    "  \"parent_id\": \"UUID...\",\n",
    "  \"proxy\": \"...\",\n",
    "  \"model\": \"gpt-4\", // gpt-4-browsing, text-davinci-002-render-sha, gpt-4, gpt-4-plugins\n",
    "  \"plugin_ids\" : [\"plugin-d1d6eb04-3375-40aa-940a-c2fc57ce0f51\"], // Wolfram Alpha example\n",
    "  \"disable_history\": true,\n",
    "}\n",
    "```\n",
    "1. Save this as $HOME/.config/revChatGPT/config.json\n",
    "2. If you are using Windows, you will need to create an environment variable named HOME and set it to your home profile for the script to be able to locate the config.json file.\n",
    "\n",
    "Plugin IDs can be found [here](https://github.com/acheong08/ChatGPT/blob/main/docs/plugins.json). Remember to set model to gpt-4-plugins if plugins are enabled. Plugins may or may not work if you haven't installed them from the web interface. You can call chatbot.install_plugin(plugin_id=plugin_id) to install any one of them from code. Call chatbot.get_plugins() to get a list of all plugins available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "\n",
      "\n",
      "gpt-3.5-turbo\n",
      "Total length: 10 tokens. Length:5 words\n",
      "text| example| на| раз|ных| я|з|ы|ка|х\n",
      "1342|3187|13373|39479|44786|46410|9136|4655|13433|10693\n"
     ]
    }
   ],
   "source": [
    "def num_tokens_from_messages(messages, model:str = \"cl100k_base\") -> int:\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(messages))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def print_tokens(msg:str, model:str=\"cl100k_base\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens_ids = encoding.encode(msg)\n",
    "\n",
    "    # for token_id in tokens_ids:\n",
    "    #     token = encoding.decode([token_id])\n",
    "    #     print(f\"Token ID: {token_id}, Token: {token}\")\n",
    "    print(f\"\\n\\n{model}\")\n",
    "    print(f\"Total length: {len(tokens_ids)} tokens. Length:{len(msg.split())} words\")\n",
    "    print(\"|\".join([encoding.decode([token_id]) for token_id in tokens_ids]))\n",
    "    print(\"|\".join([str(token_id) for token_id in tokens_ids]))\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "message = \"text example на разных языках\"\n",
    "\n",
    "print(num_tokens_from_messages(message, \"gpt-3.5-turbo\"))\n",
    "print_tokens(message, \"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = os.environ.get(\"CHAT_GPT_ACCESS_TOKEN\")\n",
    "home = os.environ.get(\"HOME\")\n",
    "# access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using chat.openai.com backend\n"
     ]
    }
   ],
   "source": [
    "chat_config = {\n",
    "    \"access_token\": access_token,\n",
    "    \"model\": \"gpt-4\",\n",
    "    \"disable_history\": True,\n",
    "}\n",
    "\n",
    "chatbot = Chatbot(config=chat_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI, I don't have real-time data. As of my last training cut-off in September 2021, Portugal has over 800 beaches. However, this number might vary depending on how you count them - some might count smaller, separate sections of the same coastline as different beaches, while others might count them as one. To get the most accurate and up-to-date information, please refer to a reliable source or the Portuguese tourism board.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"how many beaches does portugal have?\"\n",
    "\n",
    "response = \"\"\n",
    "for data in chatbot.ask(prompt):\n",
    "    response = data[\"message\"]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "None\n",
      "{}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(chatbot.conversation_id_prev_queue)\n",
    "print(chatbot.conversation_id)\n",
    "print(chatbot.conversation_mapping)\n",
    "print(chatbot.parent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '137ee605-79d0-4280-b7df-69c80a8b886e',\n",
       "  'title': 'Эксперименты с LangChain.',\n",
       "  'create_time': '2023-06-03T18:31:33.657340+00:00',\n",
       "  'update_time': '2023-06-03T18:52:07+00:00',\n",
       "  'mapping': None,\n",
       "  'current_node': None}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.get_conversations()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '207d1b95-c85e-4cf1-b623-5d0391d173db',\n",
       " 'title': 'Оптимизация',\n",
       " 'create_time': '2023-05-31T16:55:14.827552+00:00',\n",
       " 'update_time': '2023-06-03T19:04:37+00:00',\n",
       " 'mapping': None,\n",
       " 'current_node': None}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.get_conversations()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = chatbot.get_msg_history(\"207d1b95-c85e-4cf1-b623-5d0391d173db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'create_time', 'update_time', 'mapping', 'moderation_results', 'current_node'])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chat_history[\"mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system 0 0 \n",
      "user 371 159 [Инструкци\n",
      "assistant 2 2 Ок\n",
      "user 888 371 Мы делаем \n",
      "assistant 2 2 Ок\n",
      "user 568 165 Есть следу\n",
      "assistant 2 2 Ок\n",
      "user 8627 2043 Есть следу\n",
      "assistant 2 2 Ок\n",
      "user 61 19 Есть следу\n",
      "assistant 2 2 Ок\n",
      "user 3506 869 # constrai\n",
      "assistant 2 2 Ок\n",
      "user 5117 1172 # constrai\n",
      "assistant 2 2 Ок\n",
      "user 4893 1081 # constrai\n",
      "assistant 16 8 Ок, продол\n",
      "user 8686 2057 # constrai\n",
      "assistant 331 151 Опять же, \n",
      "user 51 16 Есть следу\n",
      "assistant 153 64 Извините з\n",
      "user 45 23 Подскажи п\n",
      "assistant 590 253 В начале э\n",
      "user 44 17 Какими был\n",
      "assistant 356 141 В самом на\n",
      "user 40 11 что было д\n",
      "assistant 324 125 Извините з\n",
      "user 43 24 какое кол-\n",
      "assistant 220 93 Извините, \n",
      "user 30 13 а если поп\n",
      "assistant 442 193 Извините з\n",
      "user 33 12 а количест\n",
      "assistant 312 121 В настоящи\n",
      "user 62 32 в начале б\n",
      "assistant 247 113 Я извиняюс\n",
      "user 38 18 Приведи пе\n",
      "assistant 496 210 К сожалени\n",
      "user 60 26 Приведи по\n",
      "assistant 489 193 К сожалени\n",
      "user 198 86 Сэм - это \n",
      "assistant 829 330 Сэм, как \"\n",
      "user 46 23 понятно. К\n",
      "assistant 410 155 Мы рассмот\n",
      "user 51 19 в скольких\n",
      "assistant 68 30 Файл `cons\n",
      "user 36 11 Какое коли\n",
      "assistant 602 232 К сожалени\n",
      "user 59 25 приведи по\n",
      "assistant 386 127 Вот первые\n",
      "user 52 20 а теперь п\n",
      "assistant 538 147 Вот послед\n",
      "user 48 17 так скольк\n",
      "assistant 350 131 Я не могу \n",
      "user 48 20 подсчитай \n",
      "assistant 285 121 Извините з\n",
      "user 42 18 приведи ве\n",
      "assistant 348 152 Извините, \n",
      "user 52 24 приведи по\n",
      "assistant 449 177 Извините, \n",
      "user 92 35 о каких фа\n",
      "assistant 601 237 В нашей бе\n",
      "user 134 45 о чем файл\n",
      "assistant 830 335 Файл 'cons\n",
      "user 38 14 приведи по\n",
      "assistant 277 100 Извините, \n"
     ]
    }
   ],
   "source": [
    "for map_id in chat_history[\"mapping\"].keys():\n",
    "    message = chat_history[\"mapping\"][map_id].get(\"message\", None)\n",
    "    if message is not None:\n",
    "        role = message[\"author\"][\"role\"]\n",
    "        metadata = message[\"author\"][\"metadata\"]\n",
    "        status = message[\"status\"]\n",
    "        weight = message[\"weight\"]\n",
    "\n",
    "        content = message[\"content\"]\n",
    "        content_type = content[\"content_type\"]\n",
    "        content_text = content[\"parts\"][0]\n",
    "        content_text_len = len(content_text)\n",
    "        content_text_tokenlen = num_tokens_from_messages(content_text, \"gpt-3.5-turbo\")\n",
    "\n",
    "        # print(role, metadata, status, weight, len(content_text))\n",
    "        print(role, content_text_len, content_text_tokenlen, content_text[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "gpt-3.5-turbo\n",
      "Total length: 165 tokens. Length:56 words\n",
      "Е|сть| след|ующ|ий| файл|:| metrics|.py|\n",
      "|С|одерж|им|ое| файла|:\n",
      "|\"\"\"|О|с|нов|ные| б|из|н|ес|ов|ые| м|ет|ри|ки|.\"\"\"\n",
      "|import| typing| as| tp|\n",
      "|import| py|omo|.environ| as| py|o|\n",
      "\n",
      "|def| objective|_rule|_n|sv|(model|:| py|o|.Con|crete|Model|)| ->| tp|.Any|:\n",
      "|   | return| sum|(model|.inc|_n|sv|[v|]| *| model|.|using|_idx|[v|]| for| v| in| model|.com|binations|)\n",
      "\n",
      "|def| objective|_rule|_l|sv|(model|:| py|o|.Con|crete|Model|)| ->| tp|.Any|:\n",
      "|   | return| sum|(model|.inc|_l|sv|[v|]| *| model|.|using|_idx|[v|]| for| v| in| model|.com|binations|)\n",
      "\n",
      "|def| objective|_rule|_up|lift|(model|:| py|o|.Con|crete|Model|)| ->| tp|.Any|:\n",
      "|   | return| sum|(model|.inc|_up|lift|[v|]| *| model|.|using|_idx|[v|]| for| v| in| model|.com|binations|)\n",
      "\n",
      "56191|35592|93129|72119|44938|47418|25|17150|7345|198|19871|75346|16494|44867|71873|512|12885|20976|2297|47805|44065|14391|41873|2156|23297|6856|51736|11562|8341|31203|17165|14781|475|20061|439|18593|198|475|4611|16279|24656|439|4611|78|271|755|16945|21808|1107|3576|7790|25|4611|78|4906|9537|1747|8|1492|18593|13614|512|262|471|2694|7790|27895|1107|3576|16936|60|353|1646|13|985|7406|16936|60|369|348|304|1646|916|74729|696|755|16945|21808|918|3576|7790|25|4611|78|4906|9537|1747|8|1492|18593|13614|512|262|471|2694|7790|27895|918|3576|16936|60|353|1646|13|985|7406|16936|60|369|348|304|1646|916|74729|696|755|16945|21808|8401|36069|7790|25|4611|78|4906|9537|1747|8|1492|18593|13614|512|262|471|2694|7790|27895|8401|36069|16936|60|353|1646|13|985|7406|16936|60|369|348|304|1646|916|74729|340\n"
     ]
    }
   ],
   "source": [
    "aaa = 'Есть следующий файл: metrics.py\\nСодержимое файла:\\n\"\"\"Основные бизнесовые метрики.\"\"\"\\nimport typing as tp\\nimport pyomo.environ as pyo\\n\\ndef objective_rule_nsv(model: pyo.ConcreteModel) -> tp.Any:\\n    return sum(model.inc_nsv[v] * model.using_idx[v] for v in model.combinations)\\n\\ndef objective_rule_lsv(model: pyo.ConcreteModel) -> tp.Any:\\n    return sum(model.inc_lsv[v] * model.using_idx[v] for v in model.combinations)\\n\\ndef objective_rule_uplift(model: pyo.ConcreteModel) -> tp.Any:\\n    return sum(model.inc_uplift[v] * model.using_idx[v] for v in model.combinations)\\n'\n",
    "print_tokens(aaa, \"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'aaa20fc0-aa97-4108-9d25-56fea02d1da6',\n",
       " 'message': {'id': 'aaa20fc0-aa97-4108-9d25-56fea02d1da6',\n",
       "  'author': {'role': 'user', 'metadata': {}},\n",
       "  'create_time': 1685552114.831427,\n",
       "  'content': {'content_type': 'text',\n",
       "   'parts': ['[Инструкции] Сейчас я буду загружать контекст задачи. После получения части информации нет необходимости развивать беседу, достаточно отвечать одним словом: \"Ок\". Если я буду передавать большой файл - я буду писать дробью какую часть файла я передаю. Например 1/4 - первая часть из четырех, 3/5 - третья часть из пяти. Отвечать развернуто можно только при прямом вопросе.']},\n",
       "  'status': 'finished_successfully',\n",
       "  'weight': 1.0,\n",
       "  'metadata': {'timestamp_': 'absolute'},\n",
       "  'recipient': 'all'},\n",
       " 'parent': '836da384-0b30-4723-a22e-ad07cfd20a8e',\n",
       " 'children': ['0cc07a10-b171-4784-b938-e066b733743d']}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history[\"mapping\"][\"aaa20fc0-aa97-4108-9d25-56fea02d1da6\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history[\"mapping\"][\"aaa20fc0-aa97-4108-9d25-56fea02d1da6\"][\"message\"][\"author\"][\"role\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rus: 159\n",
      "En: 42\n",
      "En: 35\n",
      "En: 32\n",
      "En: 27\n",
      "En: 26\n"
     ]
    }
   ],
   "source": [
    "msg = '[Инструкции] Сейчас я буду загружать контекст задачи. После получения части информации нет необходимости развивать беседу, достаточно отвечать одним словом: \"Ок\". Если я буду передавать большой файл - я буду писать дробью какую часть файла я передаю. Например 1/4 - первая часть из четырех, 3/5 - третья часть из пяти. Отвечать развернуто можно только при прямом вопросе.'\n",
    "print(\"Rus:\",num_tokens_from_messages(msg, \"gpt-3.5-turbo\"))\n",
    "\n",
    "msg = '[Instructions] Currently I will load context. Reply with only \"Ok\" when ready. For large files, I will indicate the part number (e.g. 1/4). Answer directly only for questions'\n",
    "print(\"En:\", num_tokens_from_messages(msg, \"gpt-3.5-turbo\"))\n",
    "\n",
    "msg = 'Loading context. Reply with \"Ok\" when ready. For large files, I will indicate the part number (e.g. 1/4). Answer directly only for questions'\n",
    "print(\"En:\", num_tokens_from_messages(msg, \"gpt-3.5-turbo\"))\n",
    "\n",
    "msg = 'Reply with \"Ok\" when ready. For large files, I will indicate the part number (e.g. 1/4). Answer directly only for questions'\n",
    "print(\"En:\", num_tokens_from_messages(msg, \"gpt-3.5-turbo\"))\n",
    "\n",
    "msg = \"Say 'Ok' when ready. I'll note large file parts (e.g., 1/4). Only respond fully to questions\"\n",
    "print(\"En:\", num_tokens_from_messages(msg, \"gpt-3.5-turbo\"))\n",
    "\n",
    "msg = \"Reply 'Ok' when set. For big files, I'll signal part number (like 1/4). Answer questions directly\"\n",
    "print(\"En:\", num_tokens_from_messages(msg, \"gpt-3.5-turbo\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
